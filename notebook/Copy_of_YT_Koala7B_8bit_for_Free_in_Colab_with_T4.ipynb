{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VJ7lfTYz0qu",
    "outputId": "20b4c81d-5992-4c2a-a79f-a894ad7dba1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip -q install datasets loralib sentencepiece \n",
    "!pip -q install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwn1vLOW-VQz"
   },
   "source": [
    "# Koala 7B loading in 8bit on a T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdVSk5iZ1DVB",
    "outputId": "4b042b3d-dafc-4837-d0e1-4ac2d27fc6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 19 20:17:11 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.04              Driver Version: 531.29       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090         On | 00000000:01:00.0  On |                  N/A |\n",
      "| 40%   49C    P0              132W / 370W|   1549MiB / 24576MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        26      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MK_96JPU0fgQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uns/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import torch\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lHAMGhEW0ldH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/uns/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If you are on a cluster, make sure you are on a CUDA machine!\n",
      "CUDA SETUP: Loading binary /home/uns/.local/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "  warn(msg)\n",
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('VSCODE_WSL_EXT_LOCATION/up')}\n",
      "  warn(msg)\n",
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/uns/go/bin')}\n",
      "  warn(msg)\n",
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/home/uns/.local/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No GPU detected! Check your CUDA paths. Proceeding to load CPU-only library...\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:25<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"samwit/koala-7b\")\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"samwit/koala-7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJSkqjrYva2a",
    "outputId": "c75c91e8-7a05-40cb-94d9-a5a2e100f0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 19 20:18:13 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.04              Driver Version: 531.29       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090         On | 00000000:01:00.0  On |                  N/A |\n",
      "| 58%   50C    P2              132W / 370W|   9462MiB / 24576MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        26      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A       572      C   /python3.8                                N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "odWWkO_81HIl"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=712,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_MBqSkZ1iQZ"
   },
   "source": [
    "## Run it as a HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DE_cImNYcsAy"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False, runner: str = 'google-colab'):\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        base_path = '../templates/'\n",
    "        if runner == 'google-colab':\n",
    "            base_path = '/content/'\n",
    "        \n",
    "        \n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(base_path, f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "question_prompter = Prompter(template_name=\"generate_question\", runner=\"local\")\n",
    "answer_prompter = Prompter(template_name=\"generate_answer\", runner=\"local\")\n",
    "\n",
    "\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SITCakdmiogu"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_response(text: str):\n",
    "    splitted = text.split('### Response:\\n')\n",
    "    return splitted[1].strip()\n",
    "\n",
    "\n",
    "\n",
    "def response_to_array(text: str):\n",
    "    questions = text.rsplit('\\n');\n",
    "    pattern = r'^[1|2|3]\\.\\s+'\n",
    "    qs_cleaned = [];\n",
    "    \n",
    "    for q in questions: \n",
    "        q_cleaned = re.split(pattern, q)[1]\n",
    "        qs_cleaned.append(q_cleaned)\n",
    "    \n",
    "    return qs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HuGbs96dfhu",
    "outputId": "719f7838-213a-4c4d-f368-dbb7ac0b45bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a\n",
      "response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      " Generate 3 questions for the following passage\n",
      "\n",
      "### Input:\n",
      " Within Flowbird. Smart city multi-services kiosque is a terminal which replaced a Parking meter (\"horodateur\"\n",
      "in French). The difference between a parking meter and a smart city multi-services kiosque is that last one\n",
      "offers the following functions besides the possibility to pay for parking services: maps, timetables, guides,\n",
      "advertisements, tickets for events, coupons which offer discounts in shops etc. Examples of Smart city mult-\n",
      "services kiosques are: StradaPAL and T-PAL module.\n",
      "\n",
      "### Response:\n",
      " 1.   What are some examples of additional functions offered by smart city multi-services kiosques beyond\n",
      "parking?\n",
      "2.   How do smart city multi-services kiosques differ from traditional parking meters?\n",
      "3.   What are some potential benefits of using smart city multi-services kiosques over traditional parking\n",
      "meters?\n"
     ]
    }
   ],
   "source": [
    "context= 'Smart city multi-services kiosque is a terminal which replaced a Parking meter (\"horodateur\" in French). The difference between a parking meter and a smart city multi-services kiosque is that last one offers the following functions besides the possibility to pay for parking services: maps, timetables, guides, advertisements, tickets for events, coupons which offer discounts in shops etc. Examples of Smart city mult-services kiosques are: StradaPAL and T-PAL module.'\n",
    "q_prompt = question_prompter.generate_prompt(\n",
    "    '',\n",
    "    context\n",
    ")\n",
    "output = pipe(q_prompt)\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))\n",
    "\n",
    "questions = extract_response(output[0]['generated_text'])\n",
    "\n",
    "questions_array = response_to_array(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Below is an instruction that describes a context. Answer 3 questions from input base on the provided context.\n",
      "\n",
      "### Instruction:\n",
      " Smart city multi-services kiosque is a terminal which replaced a Parking meter (\"horodateur\" in French). The\n",
      "difference between a parking meter and a smart city multi-services kiosque is that last one offers the\n",
      "following functions besides the possibility to pay for parking services: maps, timetables, guides,\n",
      "advertisements, tickets for events, coupons which offer discounts in shops etc. Examples of Smart city mult-\n",
      "services kiosques are: StradaPAL and T-PAL module.\n",
      "\n",
      "### Input:\n",
      "1.   What are some examples of additional functions offered by smart city multi-services kiosques beyond\n",
      "parking?\n",
      "2.   How do smart city multi-services kiosques differ from traditional parking meters?\n",
      "3.   What are some potential benefits of using smart city multi-services kiosques over traditional parking\n",
      "meters?\n",
      "\n",
      "### Response:\n",
      "\n",
      "1.   Some examples of additional functions offered by smart city multi-services kiosques beyond parking\n",
      "include maps, timetables, guides, advertisements, and tickets for events.\n",
      "2.   Smart city multi-services kiosques differ from traditional parking meters in that they offer additional\n",
      "functions beyond just the ability to pay for parking services.\n",
      "3.   Potential benefits of using smart city multi-services kiosques over traditional parking meters include\n",
      "convenience, accessibility, and improved customer experience.\n"
     ]
    }
   ],
   "source": [
    "a_prompt = answer_prompter.generate_prompt(\n",
    "    context,\n",
    "    questions\n",
    ")\n",
    "output = pipe(a_prompt)\n",
    "print('\\n\\n')\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))\n",
    "\n",
    "answers = extract_response(output[0]['generated_text'])\n",
    "\n",
    "answers_array = response_to_array(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5QHbFVst4Ih"
   },
   "source": [
    "extract question array\n",
    "extract answer array\n",
    "\n",
    "make JSON\n",
    "\n",
    "for \n",
    "\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"${question}\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"${answer}\"\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are some examples of additional functions offered by smart city multi-services kiosques beyond parking?', 'How do smart city multi-services kiosques differ from traditional parking meters?', 'What are some potential benefits of using smart city multi-services kiosques over traditional parking meters?']\n",
      "['Some examples of additional functions offered by smart city multi-services kiosques beyond parking include maps, timetables, guides, advertisements, and tickets for events.', 'Smart city multi-services kiosques differ from traditional parking meters in that they offer additional functions beyond just the ability to pay for parking services.', 'Potential benefits of using smart city multi-services kiosques over traditional parking meters include convenience, accessibility, and improved customer experience.']\n",
      "[{'instruction': 'What are some examples of additional functions offered by smart city multi-services kiosques beyond parking?', 'input': 'In context of Flowbird Group', 'output': 'Some examples of additional functions offered by smart city multi-services kiosques beyond parking include maps, timetables, guides, advertisements, and tickets for events.'}, {'instruction': 'How do smart city multi-services kiosques differ from traditional parking meters?', 'input': 'In context of Flowbird Group', 'output': 'Smart city multi-services kiosques differ from traditional parking meters in that they offer additional functions beyond just the ability to pay for parking services.'}, {'instruction': 'What are some potential benefits of using smart city multi-services kiosques over traditional parking meters?', 'input': 'In context of Flowbird Group', 'output': 'Potential benefits of using smart city multi-services kiosques over traditional parking meters include convenience, accessibility, and improved customer experience.'}]\n"
     ]
    }
   ],
   "source": [
    "print(questions_array)\n",
    "\n",
    "print(answers_array)\n",
    "\n",
    "\n",
    "for i, q in enumerate(questions_array):\n",
    "    data = {\n",
    "        \"instruction\": questions_array[i],\n",
    "        \"input\": \"In context of Flowbird Group\",\n",
    "        \"output\": answers_array[i]\n",
    "    }\n",
    "    dataset.append(data);\n",
    "\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cik7EXzWfWi3"
   },
   "source": [
    "OLD STUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGGmYmTC31om"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: \\\n",
    "What are the difference between Llamas, Alpacas and Koalas?')\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R846mNA-EPJl"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: \\\n",
    "Write a short note to Sam Altman giving reasons to open source GPT-4')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etXw2n6mD_4e"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: What is the capital of England? \\n')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pE2hSvhsEic",
    "outputId": "b710451a-cabd-4d21-fb16-de8e1ac4943d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGINNING OF CONVERSATION: USER: Write a story about a Koala playing pool and beating all the camelids.\n",
      "Once upon a time, in a far-off land where animals could talk, there lived a koala named Koko. Koko was known\n",
      "for his love of playing pool and his exceptional skills at it. One day, he decided to challenge all the other\n",
      "animals in the kingdom to a game of pool.\n",
      "\n",
      "The first animal he challenged was a camel named Cammy. Cammy accepted the challenge and they played a match.\n",
      "At first, Koko thought he had an easy win as he had been practicing for weeks. But to his surprise, Cammy\n",
      "proved to be a formidable opponent. The two played several games back and forth, with neither one able to gain\n",
      "the upper hand.\n",
      "\n",
      "Next up was a llama named Lola. She too accepted the challenge and they played a match. Once again, Koko\n",
      "thought he would have an easy win but Lola's quick reflexes and strategic thinking made her a tough opponent.\n",
      "They played several games back and forth, with both players giving their best effort.\n",
      "\n",
      "Finally, Koko faced off against a group of alpacas who were also skilled at pool. He thought he had them beat,\n",
      "but they too proved to be fierce competitors. The match went on for hours, with no clear winner until finally,\n",
      "after much debate and intense competition, Koko emerged victorious.\n",
      "\n",
      "From that day forward, Koko became known throughout the kingdom as the greatest pool player among the animals.\n",
      "And every time he saw any of the other animals playing pool, he couldn't help but smile to himself, knowing\n",
      "that he was still the king of the game.\n",
      "CPU times: user 1min 6s, sys: 176 ms, total: 1min 6s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: Write a story about a Koala playing pool and beating all the camelids.')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQTgOX168NeK",
    "outputId": "e7961685-d56e-4808-d659-7eac538e8d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGINNING OF CONVERSATION: USER: As an AI do you like the Simpsons? What dow you know about Homer?\n",
      "As an AI, I don't have personal preferences or feelings. However, I can tell you that The Simpsons is a\n",
      "popular animated television show created by Matt Groening. It first aired in 1989 and has since become one of\n",
      "the longest-running TV shows in history. Homer Simpson is one of the main characters on the show, he is a\n",
      "lazy, accident-prone father who often gets himself into trouble with his family and friends. He is known for\n",
      "his catchphrase \"Doh!\" and his love for beer.\n",
      "CPU times: user 26 s, sys: 74.6 ms, total: 26.1 s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: As an AI do you like the Simpsons? What dow you know about Homer?')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Nil30lVDLHr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
