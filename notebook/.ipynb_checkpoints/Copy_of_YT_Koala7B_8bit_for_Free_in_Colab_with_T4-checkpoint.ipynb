{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VJ7lfTYz0qu",
    "outputId": "20b4c81d-5992-4c2a-a79f-a894ad7dba1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip -q install datasets loralib sentencepiece \n",
    "!pip -q install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwn1vLOW-VQz"
   },
   "source": [
    "# Koala 7B loading in 8bit on a T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdVSk5iZ1DVB",
    "outputId": "4b042b3d-dafc-4837-d0e1-4ac2d27fc6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 18 17:25:39 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MK_96JPU0fgQ"
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "import torch\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHAMGhEW0ldH"
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"samwit/koala-7b\")\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"samwit/koala-7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJSkqjrYva2a",
    "outputId": "c75c91e8-7a05-40cb-94d9-a5a2e100f0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  7 13:21:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   60C    P0    28W /  70W |   8149MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odWWkO_81HIl"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=712,\n",
    "    temperature=0.3,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_MBqSkZ1iQZ"
   },
   "source": [
    "## Run it as a HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE_cImNYcsAy"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "class Prompter(object, runner='google-colab'):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        base_path = 'templates/'\n",
    "        if (runner == 'google-colab') {\n",
    "            base_path = '/content/'\n",
    "        }\n",
    "        \n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"/content/\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "question_prompter = Prompter(\"generate_question\")\n",
    "answer_prompter = Prompter(\"generate_answer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SITCakdmiogu"
   },
   "outputs": [],
   "source": [
    "def extract_question(text: str):\n",
    "    splitted = text.split('### Response:\\n')\n",
    "    return splitted[1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HuGbs96dfhu",
    "outputId": "719f7838-213a-4c4d-f368-dbb7ac0b45bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a\n",
      "response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      " Generate 3 questions for the following passage\n",
      "\n",
      "### Input:\n",
      " Within Flowbird. Smart city multi-services kiosque is a terminal which replaced a Parking meter (\"horodateur\"\n",
      "in French). The difference between a parking meter and a smart city multi-services kiosque is that last one\n",
      "offers the following functions besides the possibility to pay for parking services: maps, timetables, guides,\n",
      "advertisements, tickets for events, coupons which offer discounts in shops etc. Examples of Smart city mult-\n",
      "services kiosques are: StradaPAL and T-PAL module.\n",
      "\n",
      "### Response:\n",
      " 1.   What are some examples of additional functions offered by smart city multi-services kiosques beyond\n",
      "parking?\n",
      "2.   How do these additional functions benefit users compared to traditional parking meters?\n",
      "3.   What are some potential challenges or limitations of implementing smart city multi-services kiosques in\n",
      "urban environments?\n",
      "\n",
      "\n",
      "\n",
      "Below is an instruction that describes a context. Answer 3 questions from input base on the provided context.\n",
      "\n",
      "### Instruction:\n",
      " Smart city multi-services kiosque is a terminal which replaced a Parking meter (\"horodateur\" in French). The\n",
      "difference between a parking meter and a smart city multi-services kiosque is that last one offers the\n",
      "following functions besides the possibility to pay for parking services: maps, timetables, guides,\n",
      "advertisements, tickets for events, coupons which offer discounts in shops etc. Examples of Smart city mult-\n",
      "services kiosques are: StradaPAL and T-PAL module.\n",
      "\n",
      "### Input:\n",
      "1.   What are some examples of additional functions offered by smart city multi-services kiosques beyond\n",
      "parking?\n",
      "2.   How do these additional functions benefit users compared to traditional parking meters?\n",
      "3.   What are some potential challenges or limitations of implementing smart city multi-services kiosques in\n",
      "urban environments?\n",
      "\n",
      "### Response:\n",
      "\n",
      "1.   Some examples of additional functions offered by smart city multi-services kiosques beyond parking\n",
      "include maps, timetables, guides, advertisements, and tickets for events. These functions provide users with\n",
      "more information and convenience than traditional parking meters.\n",
      "2.   These additional functions benefit users by providing them with access to a wider range of services and\n",
      "information, as well as the ability to purchase tickets and make reservations without having to visit a\n",
      "separate location. They also allow businesses to reach a larger audience and increase their revenue through\n",
      "advertising and ticket sales.\n",
      "3.   Potential challenges or limitations of implementing smart city multi-services kiosques in urban\n",
      "environments may include high installation costs, limited space for multiple kiosques, and competition from\n",
      "other forms of transportation and entertainment. Additionally, there may be concerns about privacy and\n",
      "security, as well as the potential for vandalism or theft.\n"
     ]
    }
   ],
   "source": [
    "context= 'Smart city multi-services kiosque is a terminal which replaced a Parking meter (\"horodateur\" in French). The difference between a parking meter and a smart city multi-services kiosque is that last one offers the following functions besides the possibility to pay for parking services: maps, timetables, guides, advertisements, tickets for events, coupons which offer discounts in shops etc. Examples of Smart city mult-services kiosques are: StradaPAL and T-PAL module.'\n",
    "q_prompt = question_prompter.generate_prompt(\n",
    "    '',\n",
    "    context\n",
    ")\n",
    "output = pipe(q_prompt)\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))\n",
    "\n",
    "questions = extract_question(output[0]['generated_text'])\n",
    "\n",
    "a_prompt = answer_prompter.generate_prompt(\n",
    "    context,\n",
    "    questions\n",
    ")\n",
    "output = pipe(a_prompt)\n",
    "print('\\n\\n')\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5QHbFVst4Ih"
   },
   "source": [
    "extract question array\n",
    "extract answer array\n",
    "\n",
    "make JSON\n",
    "\n",
    "for \n",
    "\n",
    "```\n",
    "{\n",
    "    \"instruction\": \"${question}\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"${answer}\"\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cik7EXzWfWi3"
   },
   "source": [
    "OLD STUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGGmYmTC31om"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: \\\n",
    "What are the difference between Llamas, Alpacas and Koalas?')\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R846mNA-EPJl"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: \\\n",
    "Write a short note to Sam Altman giving reasons to open source GPT-4')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etXw2n6mD_4e"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: What is the capital of England? \\n')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pE2hSvhsEic",
    "outputId": "b710451a-cabd-4d21-fb16-de8e1ac4943d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGINNING OF CONVERSATION: USER: Write a story about a Koala playing pool and beating all the camelids.\n",
      "Once upon a time, in a far-off land where animals could talk, there lived a koala named Koko. Koko was known\n",
      "for his love of playing pool and his exceptional skills at it. One day, he decided to challenge all the other\n",
      "animals in the kingdom to a game of pool.\n",
      "\n",
      "The first animal he challenged was a camel named Cammy. Cammy accepted the challenge and they played a match.\n",
      "At first, Koko thought he had an easy win as he had been practicing for weeks. But to his surprise, Cammy\n",
      "proved to be a formidable opponent. The two played several games back and forth, with neither one able to gain\n",
      "the upper hand.\n",
      "\n",
      "Next up was a llama named Lola. She too accepted the challenge and they played a match. Once again, Koko\n",
      "thought he would have an easy win but Lola's quick reflexes and strategic thinking made her a tough opponent.\n",
      "They played several games back and forth, with both players giving their best effort.\n",
      "\n",
      "Finally, Koko faced off against a group of alpacas who were also skilled at pool. He thought he had them beat,\n",
      "but they too proved to be fierce competitors. The match went on for hours, with no clear winner until finally,\n",
      "after much debate and intense competition, Koko emerged victorious.\n",
      "\n",
      "From that day forward, Koko became known throughout the kingdom as the greatest pool player among the animals.\n",
      "And every time he saw any of the other animals playing pool, he couldn't help but smile to himself, knowing\n",
      "that he was still the king of the game.\n",
      "CPU times: user 1min 6s, sys: 176 ms, total: 1min 6s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: Write a story about a Koala playing pool and beating all the camelids.')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQTgOX168NeK",
    "outputId": "e7961685-d56e-4808-d659-7eac538e8d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGINNING OF CONVERSATION: USER: As an AI do you like the Simpsons? What dow you know about Homer?\n",
      "As an AI, I don't have personal preferences or feelings. However, I can tell you that The Simpsons is a\n",
      "popular animated television show created by Matt Groening. It first aired in 1989 and has since become one of\n",
      "the longest-running TV shows in history. Homer Simpson is one of the main characters on the show, he is a\n",
      "lazy, accident-prone father who often gets himself into trouble with his family and friends. He is known for\n",
      "his catchphrase \"Doh!\" and his love for beer.\n",
      "CPU times: user 26 s, sys: 74.6 ms, total: 26.1 s\n",
      "Wall time: 30.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "output = pipe('BEGINNING OF CONVERSATION: USER: As an AI do you like the Simpsons? What dow you know about Homer?')\n",
    "\n",
    "print(wrap_text_preserve_newlines(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Nil30lVDLHr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
